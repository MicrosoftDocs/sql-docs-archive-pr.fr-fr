---
title: Sélection des fonctionnalités (exploration de données) | Microsoft Docs
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 08/04/2020
ms.locfileid: "87611408"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="65c8f-102">Sélection des fonctionnalités (exploration de données)</span><span class="sxs-lookup"><span data-stu-id="65c8f-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="65c8f-103">La *sélection des fonctionnalités* est un terme couramment utilisé dans l’exploration de données pour décrire les outils et les techniques disponibles pour réduire les entrées à une taille gérable pour le traitement et l’analyse.</span><span class="sxs-lookup"><span data-stu-id="65c8f-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="65c8f-104">La sélection des caractéristiques implique non seulement une réduction de la *cardinalité*, ce qui signifie imposer une coupure arbitraire ou prédéfinie sur le nombre d’attributs qui peuvent être pris en compte lors de la création d’un modèle, mais également le choix des attributs, ce qui signifie que l’analyste ou l’outil de modélisation sélectionne ou ignore activement les attributs en fonction de leur utilité pour l’analyse.</span><span class="sxs-lookup"><span data-stu-id="65c8f-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="65c8f-105">La possibilité d'appliquer la sélection des fonctionnalités est essentielle pour une analyse efficace, car les datasets contiennent souvent beaucoup plus d'informations que nécessaire pour générer le modèle.</span><span class="sxs-lookup"><span data-stu-id="65c8f-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="65c8f-106">Par exemple, un dataset peut contenir 500 colonnes qui décrivent les caractéristiques des clients, mais si les données de certaines colonnes sont très éparses, vous tireriez peu de bénéfices à les ajouter au modèle.</span><span class="sxs-lookup"><span data-stu-id="65c8f-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="65c8f-107">Si vous gardez les colonnes inutiles pendant la génération du modèle, plus d'UC et de mémoire sont nécessaires au cours du processus d'apprentissage, et plus d'espace de stockage est requis pour le modèle terminé.</span><span class="sxs-lookup"><span data-stu-id="65c8f-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="65c8f-108">Même si les ressources sont suffisantes, en général il est préférable de supprimer les colonnes inutiles car elles peuvent dégrader la qualité des modèles découverts, pour les raisons suivantes :</span><span class="sxs-lookup"><span data-stu-id="65c8f-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="65c8f-109">Certaines colonnes sont parasites ou redondantes,</span><span class="sxs-lookup"><span data-stu-id="65c8f-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="65c8f-110">ce qui rend plus difficile la découverte de séquences significatives à partir des données.</span><span class="sxs-lookup"><span data-stu-id="65c8f-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="65c8f-111">Pour découvrir des séquences de qualité, la plupart des algorithmes d'exploration de données nécessitent un jeu de données d'apprentissage beaucoup plus important dans le cas d'un jeu de données de grande dimension.</span><span class="sxs-lookup"><span data-stu-id="65c8f-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="65c8f-112">Mais, dans certaines applications d'exploration de données, les données d'apprentissage sont peu importantes.</span><span class="sxs-lookup"><span data-stu-id="65c8f-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="65c8f-113">Si seules 50 de 500 colonnes de la source de données possèdent des informations utiles à la création d'un modèle, vous pouvez tout simplement les laisser hors du modèle, ou vous pouvez utiliser des techniques de sélection de fonctionnalités afin de découvrir automatiquement les meilleures fonctionnalités et exclure les valeurs qui sont statistiquement non significatives.</span><span class="sxs-lookup"><span data-stu-id="65c8f-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="65c8f-114">La sélection des fonctionnalités permet à la fois d'éviter d'avoir trop de données qui présentent peu d'intérêt, ou de n'avoir pas assez de données de valeur.</span><span class="sxs-lookup"><span data-stu-id="65c8f-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="65c8f-115">Sélection des fonctionnalités pour l'exploration de données Analysis Services</span><span class="sxs-lookup"><span data-stu-id="65c8f-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="65c8f-116">Habituellement, la sélection des fonctionnalités est effectuée automatiquement dans [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], et chaque algorithme comporte un ensemble de techniques par défaut permettant d'appliquer intelligemment la réduction des fonctionnalités.</span><span class="sxs-lookup"><span data-stu-id="65c8f-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="65c8f-117">La sélection des fonctionnalités est toujours effectuée avant l'apprentissage du modèle, afin de choisir automatiquement les attributs d'un dataset qui sont le plus susceptibles d'être utilisés dans le modèle.</span><span class="sxs-lookup"><span data-stu-id="65c8f-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="65c8f-118">Toutefois, vous pouvez également définir manuellement des paramètres pour influencer le comportement de la sélection des fonctionnalités.</span><span class="sxs-lookup"><span data-stu-id="65c8f-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="65c8f-119">En général, la sélection des fonctionnalités calcule un score pour chaque attribut, puis retient uniquement les attributs qui ont les meilleurs scores.</span><span class="sxs-lookup"><span data-stu-id="65c8f-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="65c8f-120">Vous pouvez également ajuster le seuil pour les scores les plus élevés.</span><span class="sxs-lookup"><span data-stu-id="65c8f-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] <span data-ttu-id="65c8f-121">fournit plusieurs méthodes pour calculer les scores ; la méthode exacte appliquée dans un modèle dépend de ces facteurs :</span><span class="sxs-lookup"><span data-stu-id="65c8f-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="65c8f-122">Algorithme utilisé dans votre modèle</span><span class="sxs-lookup"><span data-stu-id="65c8f-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="65c8f-123">Type de données de l'attribut</span><span class="sxs-lookup"><span data-stu-id="65c8f-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="65c8f-124">Tous les paramètres que vous pouvez avoir définis sur votre modèle</span><span class="sxs-lookup"><span data-stu-id="65c8f-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="65c8f-125">La sélection des fonctionnalités est appliquée aux entrées, aux attributs prévisibles ou aux états d'une colonne.</span><span class="sxs-lookup"><span data-stu-id="65c8f-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="65c8f-126">Lorsque le calcul du score de sélection des fonctionnalités est terminé, seuls les attributs et les états que l'algorithme sélectionne sont inclus dans le processus de génération de modèle et peuvent être utilisés pour des prédictions.</span><span class="sxs-lookup"><span data-stu-id="65c8f-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="65c8f-127">Si vous choisissez un attribut prédictible qui ne correspond pas au seuil pour la sélection des fonctionnalités, l'attribut peut toujours être utilisé pour la prédiction, mais les prédictions seront uniquement basées sur les statistiques globales qui existent dans le modèle.</span><span class="sxs-lookup"><span data-stu-id="65c8f-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="65c8f-128">La sélection des fonctionnalités affecte uniquement les colonnes utilisées dans le modèle et n'a aucun effet sur le stockage de la structure d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="65c8f-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="65c8f-129">Les colonnes que vous omettez dans le modèle d'exploration de données restent disponibles dans la structure, et les données contenues dans les colonnes de structure d'exploration de données sont mises en cache.</span><span class="sxs-lookup"><span data-stu-id="65c8f-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="65c8f-130">Définition de méthodes de sélection des fonctionnalités</span><span class="sxs-lookup"><span data-stu-id="65c8f-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="65c8f-131">Plusieurs méthodes s'offrent à vous pour implémenter la sélection des fonctionnalités, en fonction du type de données que vous utilisez et de l'algorithme que vous choisissez pour l'analyse.</span><span class="sxs-lookup"><span data-stu-id="65c8f-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="65c8f-132">SQL Server Analysis Services fournit plusieurs méthodes bien établies et très utilisées pour calculer les scores des attributs.</span><span class="sxs-lookup"><span data-stu-id="65c8f-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="65c8f-133">La méthode appliquée dans tout algorithme ou jeu de données dépend des types de données et de l'utilisation des colonnes.</span><span class="sxs-lookup"><span data-stu-id="65c8f-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="65c8f-134">Le score *d’intérêt et de pertinence* est utilisé pour classer par ordre de priorité et trier les attributs dans des colonnes qui contiennent des données numériques continues non binaires.</span><span class="sxs-lookup"><span data-stu-id="65c8f-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="65c8f-135">*L’entropie de Shannon* et deux scores *bayésiens* sont disponibles pour les colonnes qui contiennent des données discrètes et discrétisées.</span><span class="sxs-lookup"><span data-stu-id="65c8f-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="65c8f-136">Toutefois, si le modèle contient des colonnes continues, le score d'intérêt et de pertinence est utilisé pour évaluer toutes les colonnes d'entrée, afin de garantir la cohérence.</span><span class="sxs-lookup"><span data-stu-id="65c8f-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="65c8f-137">La section suivante décrit chaque méthode de sélection des fonctionnalités.</span><span class="sxs-lookup"><span data-stu-id="65c8f-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="65c8f-138">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="65c8f-138">Interestingness score</span></span>  
 <span data-ttu-id="65c8f-139">Une fonctionnalité présente un intérêt si elle fournit des informations utiles.</span><span class="sxs-lookup"><span data-stu-id="65c8f-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="65c8f-140">Étant donné que la définition de ce qui est utile varie selon le scénario, le secteur de l’exploration de données a développé différentes façons de mesurer l' *intérêt*.</span><span class="sxs-lookup"><span data-stu-id="65c8f-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="65c8f-141">Par exemple, la *nouveauté* peut être intéressante dans la détection de valeurs hors norme, mais la possibilité de faire la distinction entre des éléments étroitement liés, ou un *poids distinctif*, peut être plus intéressante pour la classification.</span><span class="sxs-lookup"><span data-stu-id="65c8f-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="65c8f-142">La mesure d’intérêt qui est utilisée dans SQL Server Analysis Services est *basée sur l’entropie*, ce qui signifie que les attributs avec des distributions aléatoires ont une entropie plus élevée et un gain d’informations plus faible. par conséquent, ces attributs sont moins intéressants.</span><span class="sxs-lookup"><span data-stu-id="65c8f-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="65c8f-143">L'entropie pour un attribut particulier est comparée à l'entropie de tous les autres attributs, comme suit :</span><span class="sxs-lookup"><span data-stu-id="65c8f-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="65c8f-144">Intérêt/Pertinence(Attribut) = - (m - Entropie(Attribut)) \* (m - Entropie(Attribut))</span><span class="sxs-lookup"><span data-stu-id="65c8f-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="65c8f-145">L’entropie centrale, ou m, représente l’entropie de l’ensemble du jeu de fonctionnalités.</span><span class="sxs-lookup"><span data-stu-id="65c8f-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="65c8f-146">En soustrayant l'entropie de l'attribut cible de l'entropie centrale, vous pouvez évaluer la quantité d'informations fournies par l'attribut.</span><span class="sxs-lookup"><span data-stu-id="65c8f-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="65c8f-147">Ce score est utilisé par défaut chaque fois que la colonne contient des données numériques continues non binaires.</span><span class="sxs-lookup"><span data-stu-id="65c8f-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="65c8f-148">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="65c8f-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="65c8f-149">L'entropie de Shannon mesure l'incertitude d'une variable aléatoire pour un résultat particulier.</span><span class="sxs-lookup"><span data-stu-id="65c8f-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="65c8f-150">Par exemple, l'entropie d'une partie de pile ou face peut être représentée comme une fonction de la probabilité de la pièce tombant côté pile.</span><span class="sxs-lookup"><span data-stu-id="65c8f-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="65c8f-151">Analysis Services utilise la formule suivante pour calculer l'entropie de Shannon :</span><span class="sxs-lookup"><span data-stu-id="65c8f-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="65c8f-152">H(X) = -∑ P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="65c8f-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="65c8f-153">Cette méthode de calcul de score est disponible pour les attributs discrets et discrétisés.</span><span class="sxs-lookup"><span data-stu-id="65c8f-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="65c8f-154">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="65c8f-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="65c8f-155">Analysis Services fournit deux scores de sélection des fonctionnalités qui sont basés sur les réseaux bayésiens.</span><span class="sxs-lookup"><span data-stu-id="65c8f-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="65c8f-156">Un réseau bayésien est un graphique *dirigé* ou *acyclique* d’états et de transitions entre états. Cela signifie que certains états sont toujours antérieurs à l’état actuel, que certains états sont postérieurs et que le graphique ne se répète pas ou ne fait pas de boucle.</span><span class="sxs-lookup"><span data-stu-id="65c8f-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="65c8f-157">Par définition, les réseaux bayésiens autorisent l'utilisation de connaissances antérieures.</span><span class="sxs-lookup"><span data-stu-id="65c8f-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="65c8f-158">Toutefois, la question de savoir quels états antérieurs utiliser pour le calcul des probabilités des états ultérieurs est importante pour la conception, les performances et la précision de l'algorithme.</span><span class="sxs-lookup"><span data-stu-id="65c8f-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="65c8f-159">Développé par Cooper et Herskovits, l'algorithme K2 pour l'apprentissage à partir d'un réseau bayésien est couramment employé dans le cadre de l'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="65c8f-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="65c8f-160">Cet algorithme est évolutif et peut analyser plusieurs variables, mais il requiert le tri des variables utilisées comme entrée.</span><span class="sxs-lookup"><span data-stu-id="65c8f-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="65c8f-161">Pour plus d’informations, consultez la documentation [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) de Chickering, Geiger et Heckerman.</span><span class="sxs-lookup"><span data-stu-id="65c8f-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="65c8f-162">Cette méthode de calcul de score est disponible pour les attributs discrets et discrétisés.</span><span class="sxs-lookup"><span data-stu-id="65c8f-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="65c8f-163">Équivalent bayésien de Dirichlet avec a priori uniforme</span><span class="sxs-lookup"><span data-stu-id="65c8f-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="65c8f-164">Le score d'équivalent bayésien de Dirichlet (BDE) utilise également l'analyse bayésienne pour évaluer un réseau pour un dataset donné.</span><span class="sxs-lookup"><span data-stu-id="65c8f-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="65c8f-165">La méthode de calcul de score BDE a été développée par Heckerman et est basée sur la métrique BD développée par Cooper et Herskovits.</span><span class="sxs-lookup"><span data-stu-id="65c8f-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="65c8f-166">La distribution de Dirichlet est une distribution multinomiale qui décrit la probabilité conditionnelle de chaque variable dans le réseau et qui possède de nombreuses propriétés utiles pour l'apprentissage.</span><span class="sxs-lookup"><span data-stu-id="65c8f-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="65c8f-167">L'équivalent bayésien de Dirichlet avec a priori uniforme (BDEU) suppose un cas spécial de la distribution de Dirichlet dans laquelle une constante mathématique est utilisée pour créer une distribution fixe ou uniforme d'états antérieurs.</span><span class="sxs-lookup"><span data-stu-id="65c8f-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="65c8f-168">Le score BDE suppose aussi l'équivalence de vraisemblance, ce qui signifie que les données ne sont pas censées faire la distinction entre des structures équivalentes.</span><span class="sxs-lookup"><span data-stu-id="65c8f-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="65c8f-169">En d’autres termes, si le score pour If A Then B est le même que le score pour If B Then A, les structures ne peuvent pas être différenciées sur la base des données, et la causalité ne peut pas être déduite.</span><span class="sxs-lookup"><span data-stu-id="65c8f-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="65c8f-170">Pour plus d’informations sur les réseaux bayésiens et l’implémentation de ces méthodes de calcul de scores, consultez la documentation [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span><span class="sxs-lookup"><span data-stu-id="65c8f-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="65c8f-171">Méthodes de sélection des fonctionnalités utilisées par les algorithmes Analysis Services</span><span class="sxs-lookup"><span data-stu-id="65c8f-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="65c8f-172">Le tableau suivant répertorie les algorithmes qui prennent en charge la sélection des fonctionnalités, les méthodes de sélection des fonctionnalités utilisées par l'algorithme et les paramètres à définir pour contrôler le comportement de la sélection des fonctionnalités :</span><span class="sxs-lookup"><span data-stu-id="65c8f-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="65c8f-173">Algorithm</span><span class="sxs-lookup"><span data-stu-id="65c8f-173">Algorithm</span></span>|<span data-ttu-id="65c8f-174">Méthode d'analyse</span><span class="sxs-lookup"><span data-stu-id="65c8f-174">Method of analysis</span></span>|<span data-ttu-id="65c8f-175">Commentaires</span><span class="sxs-lookup"><span data-stu-id="65c8f-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="65c8f-176">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="65c8f-176">Naive Bayes</span></span>|<span data-ttu-id="65c8f-177">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="65c8f-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="65c8f-178">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="65c8f-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="65c8f-179">Équivalent bayésien de Dirichlet avec a priori uniforme (par défaut)</span><span class="sxs-lookup"><span data-stu-id="65c8f-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="65c8f-180">L'algorithme Microsoft Naïve Bayes accepte uniquement les attributs discrets ou discrétisés ; par conséquent, il ne peut pas utiliser le score d'intérêt et de pertinence.</span><span class="sxs-lookup"><span data-stu-id="65c8f-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="65c8f-181">Pour plus d’informations sur cet algorithme, consultez [Informations techniques de référence relatives à l’algorithme MNB (Microsoft Naive Bayes)](microsoft-naive-bayes-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-182">Arbres de décision</span><span class="sxs-lookup"><span data-stu-id="65c8f-182">Decision trees</span></span>|<span data-ttu-id="65c8f-183">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="65c8f-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="65c8f-184">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="65c8f-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="65c8f-185">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="65c8f-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="65c8f-186">Équivalent bayésien de Dirichlet avec a priori uniforme (par défaut)</span><span class="sxs-lookup"><span data-stu-id="65c8f-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="65c8f-187">Si des colonnes contiennent des valeurs continues non binaires, le score d'intérêt et de pertinence est utilisé pour toutes les colonnes afin de garantir la cohérence.</span><span class="sxs-lookup"><span data-stu-id="65c8f-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="65c8f-188">Sinon, la méthode de sélection des fonctionnalités par défaut ou la méthode que vous avez spécifiée lors de la création du modèle est utilisée.</span><span class="sxs-lookup"><span data-stu-id="65c8f-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="65c8f-189">Pour plus d’informations sur cet algorithme, consultez [Références techniques relatives à l’algorithme MDT (Microsoft Decision Trees)](microsoft-decision-trees-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-190">Réseau neuronal</span><span class="sxs-lookup"><span data-stu-id="65c8f-190">Neural network</span></span>|<span data-ttu-id="65c8f-191">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="65c8f-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="65c8f-192">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="65c8f-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="65c8f-193">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="65c8f-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="65c8f-194">Équivalent bayésien de Dirichlet avec a priori uniforme (par défaut)</span><span class="sxs-lookup"><span data-stu-id="65c8f-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="65c8f-195">L'algorithme MNN (Microsoft Neural Network, réseau neuronal de Microsoft) peut utiliser les deux méthodes de type bayésien et entropie tant que les données contiennent des colonnes continues.</span><span class="sxs-lookup"><span data-stu-id="65c8f-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="65c8f-196">Pour plus d’informations sur cet algorithme, consultez [Informations techniques de référence relatives à l’algorithme MDT (Microsoft Decision Trees)](microsoft-neural-network-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-197">MLR (Microsoft Logistic Regression)</span><span class="sxs-lookup"><span data-stu-id="65c8f-197">Logistic regression</span></span>|<span data-ttu-id="65c8f-198">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="65c8f-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="65c8f-199">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="65c8f-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="65c8f-200">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="65c8f-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="65c8f-201">Équivalent bayésien de Dirichlet avec a priori uniforme (par défaut)</span><span class="sxs-lookup"><span data-stu-id="65c8f-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="65c8f-202">Bien que l'algorithme MLR (Microsoft Logistic Regression) soit basé sur l'algorithme MNN (Microsoft Neural Network), vous ne pouvez pas personnaliser les modèles de régression logistique de façon à contrôler le comportement de la sélection des fonctionnalités ; par conséquent, la valeur par défaut de la sélection des fonctionnalités est toujours la méthode la plus appropriée pour l'attribut.</span><span class="sxs-lookup"><span data-stu-id="65c8f-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="65c8f-203">Si tous les attributs sont discrets ou discrétisés, la valeur par défaut est BDEU.</span><span class="sxs-lookup"><span data-stu-id="65c8f-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="65c8f-204">Pour plus d’informations sur cet algorithme, consultez [Informations techniques de référence relatives à l’algorithme MLR (Microsoft Logistic Regression)](microsoft-logistic-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-205">Clustering</span><span class="sxs-lookup"><span data-stu-id="65c8f-205">Clustering</span></span>|<span data-ttu-id="65c8f-206">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="65c8f-206">Interestingness score</span></span>|<span data-ttu-id="65c8f-207">L'algorithme de gestion de clusters Microsoft peut utiliser des données discrètes ou discrétisées.</span><span class="sxs-lookup"><span data-stu-id="65c8f-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="65c8f-208">Toutefois, le score de chaque attribut étant calculé en tant que distance et étant représenté sous la forme d'un nombre continu, le score d'intérêt et de pertinence doit être utilisé.</span><span class="sxs-lookup"><span data-stu-id="65c8f-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="65c8f-209">Pour plus d’informations sur cet algorithme, consultez [Informations techniques de référence relatives à l’algorithme de gestion de clusters Microsoft](microsoft-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-210">Régression linéaire</span><span class="sxs-lookup"><span data-stu-id="65c8f-210">Linear regression</span></span>|<span data-ttu-id="65c8f-211">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="65c8f-211">Interestingness score</span></span>|<span data-ttu-id="65c8f-212">L'algorithme MLR (Microsoft Linear Regression) Microsoft peut utiliser uniquement le score d'intérêt et de pertinence, car il prend en charge uniquement les colonnes continues.</span><span class="sxs-lookup"><span data-stu-id="65c8f-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="65c8f-213">Pour plus d’informations sur cet algorithme, consultez [Informations techniques de référence relatives à l’algorithme MLR (Microsoft Linear Regression)](microsoft-linear-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-214">Règles d’association</span><span class="sxs-lookup"><span data-stu-id="65c8f-214">Association rules</span></span><br /><br /> <span data-ttu-id="65c8f-215">Sequence clustering</span><span class="sxs-lookup"><span data-stu-id="65c8f-215">Sequence clustering</span></span>|<span data-ttu-id="65c8f-216">Non utilisé</span><span class="sxs-lookup"><span data-stu-id="65c8f-216">Not used</span></span>|<span data-ttu-id="65c8f-217">La sélection des fonctionnalités n'est pas appelée avec ces algorithmes.</span><span class="sxs-lookup"><span data-stu-id="65c8f-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="65c8f-218">Toutefois, vous pouvez contrôler le comportement de l'algorithme et, si nécessaire, réduire la taille des données d'entrée en définissant la valeur des paramètres MINIMUM_SUPPORT et MINIMUM_PROBABILITY.</span><span class="sxs-lookup"><span data-stu-id="65c8f-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="65c8f-219">Pour plus d’informations, consultez [Informations techniques de référence relatives à l’algorithme Microsoft Association](microsoft-association-algorithm-technical-reference.md) et [Informations techniques de référence relatives à l’algorithme MSC (Microsoft Sequence Clustering)](microsoft-sequence-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="65c8f-220">Série chronologique</span><span class="sxs-lookup"><span data-stu-id="65c8f-220">Time series</span></span>|<span data-ttu-id="65c8f-221">Non utilisé</span><span class="sxs-lookup"><span data-stu-id="65c8f-221">Not used</span></span>|<span data-ttu-id="65c8f-222">La sélection des fonctionnalités ne s'applique pas aux modèles de série chronologique.</span><span class="sxs-lookup"><span data-stu-id="65c8f-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="65c8f-223">Pour plus d’informations sur cet algorithme, consultez [Informations techniques de référence relatives à l’algorithme MTS (Microsoft Time Series)](microsoft-time-series-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="65c8f-224">Paramètres de sélection des fonctionnalités</span><span class="sxs-lookup"><span data-stu-id="65c8f-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="65c8f-225">Dans les algorithmes qui prennent en charge la sélection des fonctionnalités, vous pouvez contrôler à quel moment la sélection des fonctionnalités est activée au moyen des paramètres suivants.</span><span class="sxs-lookup"><span data-stu-id="65c8f-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="65c8f-226">Chaque algorithme a une valeur par défaut pour le nombre des entrées autorisées, mais vous pouvez remplacer cette valeur par défaut et spécifier le nombre d'attributs.</span><span class="sxs-lookup"><span data-stu-id="65c8f-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="65c8f-227">Cette section répertorie les paramètres qui sont fournis pour gérer la sélection des fonctionnalités.</span><span class="sxs-lookup"><span data-stu-id="65c8f-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="65c8f-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="65c8f-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="65c8f-229">Si un modèle contient plus de colonnes que le nombre spécifié par le paramètre *MAXIMUM_INPUT_ATTRIBUTES* , l’algorithme ignore toutes les colonnes qu’il évalue comme inintéressantes.</span><span class="sxs-lookup"><span data-stu-id="65c8f-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="65c8f-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="65c8f-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="65c8f-231">De manière similaire, si un modèle contient plus de colonnes prédictibles que le nombre spécifié par le paramètre *MAXIMUM_OUTPUT_ATTRIBUTES* , l’algorithme ignore toutes les colonnes qu’il évalue comme inintéressantes.</span><span class="sxs-lookup"><span data-stu-id="65c8f-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="65c8f-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="65c8f-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="65c8f-233">Si un modèle contient plus de cas que le nombre spécifié par le paramètre *MAXIMUM_STATES* , les états les moins utilisés sont regroupés et traités comme étant manquants.</span><span class="sxs-lookup"><span data-stu-id="65c8f-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="65c8f-234">Si l'un de ces paramètres a la valeur 0, la sélection des fonctionnalités est désactivée, ce qui affecte le temps de traitement et les performances.</span><span class="sxs-lookup"><span data-stu-id="65c8f-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="65c8f-235">En plus de ces méthodes de sélection des fonctionnalités, vous pouvez améliorer la capacité de l’algorithme à identifier ou promouvoir des attributs explicites en définissant des *indicateurs de modélisation* sur le modèle ou en définissant des *indicateurs de distribution* sur la structure.</span><span class="sxs-lookup"><span data-stu-id="65c8f-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="65c8f-236">Pour plus d’informations sur ces concepts, consultez [Indicateurs de modélisation &#40;exploration de données&#41;](modeling-flags-data-mining.md) et [Distributions de colonnes &#40;exploration de données&#41;](column-distributions-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="65c8f-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="65c8f-237">Voir aussi</span><span class="sxs-lookup"><span data-stu-id="65c8f-237">See Also</span></span>  
 [<span data-ttu-id="65c8f-238">Personnaliser les modèles et les structures d'exploration de données</span><span class="sxs-lookup"><span data-stu-id="65c8f-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
