---
title: Informations techniques de référence sur l’algorithme Microsoft neuronal Network | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 08/04/2020
ms.locfileid: "87600348"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="44297-102">Microsoft Neural Network Algorithm Technical Reference</span><span class="sxs-lookup"><span data-stu-id="44297-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="44297-103">L’algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network) utilise un réseau *perceptron multicouche* , également appelé *réseau à règle delta à rétropropagation*, qui peut comporter jusqu’à trois couches de neurones, ou *perceptrons*.</span><span class="sxs-lookup"><span data-stu-id="44297-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="44297-104">une couche d'entrée, une couche masquée facultative et une couche de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="44297-105">Les réseaux neuronaux de type perceptron multicouche ne sont pas décrits en détails dans la présente documentation.</span><span class="sxs-lookup"><span data-stu-id="44297-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="44297-106">Cette rubrique explique l'implémentation de base de l'algorithme, dont la méthode utilisée pour normaliser les valeurs d'entrée et de sortie, ainsi que les méthodes de sélection de fonctionnalités utilisées pour réduire la cardinalité de l'attribut.</span><span class="sxs-lookup"><span data-stu-id="44297-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="44297-107">Cette rubrique décrit les paramètres et autres valeurs qui peuvent être utilisés pour personnaliser le comportement de l'algorithme, et fournit des liens vers des informations supplémentaires sur l'interrogation du modèle.</span><span class="sxs-lookup"><span data-stu-id="44297-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="44297-108">Implémentation de l'algorithme MNN (Microsoft Neural Network)</span><span class="sxs-lookup"><span data-stu-id="44297-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="44297-109">Dans un réseau neuronal de type perceptron multicouche, chaque neurone reçoit une ou plusieurs entrées et génère une ou plusieurs sorties identiques.</span><span class="sxs-lookup"><span data-stu-id="44297-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="44297-110">Chaque sortie est une fonction non linéaire simple de la somme des entrées dans le neurone.</span><span class="sxs-lookup"><span data-stu-id="44297-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="44297-111">Les entrées sont propagées vers l'avant entre les nœuds de la couche d'entrée et ceux de la couche masquée, puis entre la couche masquée et celle de sortie ; il n'y a pas de connexion entre les neurones au sein d'une même couche.</span><span class="sxs-lookup"><span data-stu-id="44297-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="44297-112">S'il n'y a pas de couche masquée, comme c'est le cas dans un modèle de régression logistique, les entrées sont propagées vers l'avant entre les nœuds de la couche d'entrée et ceux de la couche de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="44297-113">Un réseau neuronal créé avec l'algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network) comporte trois types de neurones :</span><span class="sxs-lookup"><span data-stu-id="44297-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="44297-114">Les neurones d'entrée fournissent les valeurs des attributs d'entrée du modèle d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="44297-115">Pour les attributs d'entrée discrets, un neurone d'entrée représente généralement un état unique de l'attribut d'entrée.</span><span class="sxs-lookup"><span data-stu-id="44297-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="44297-116">Cela inclut des valeurs manquantes, si les données d'apprentissage contiennent des valeurs Null pour cet attribut.</span><span class="sxs-lookup"><span data-stu-id="44297-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="44297-117">Un attribut d'entrée discret qui a plus de deux états génère un neurone d'entrée pour chaque état et un neurone d'entrée pour un état manquant, si les données d'apprentissage contiennent des valeurs Null.</span><span class="sxs-lookup"><span data-stu-id="44297-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="44297-118">Un attribut d'entrée continu génère deux neurones d'entrée : un pour un état manquant et un pour la valeur de l'attribut continu lui-même.</span><span class="sxs-lookup"><span data-stu-id="44297-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="44297-119">Les neurones d'entrée fournissent des entrées à un ou plusieurs neurones cachés.</span><span class="sxs-lookup"><span data-stu-id="44297-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="44297-120">Les neurones cachés reçoivent des entrées des neurones d'entrée et fournissent des sorties aux neurones de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="44297-121">Les neurones de sortie représentent les valeurs des attributs prédictibles du modèle d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="44297-122">Pour les attributs d'entrée discrets, un neurone de sortie représente généralement un état prédit unique d'un attribut prédictible, y compris un état manquant.</span><span class="sxs-lookup"><span data-stu-id="44297-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="44297-123">Par exemple, un attribut prédictible binaire produit un nœud de sortie qui décrit un état existant ou manquant pour indiquer si une valeur existe ou non pour cet attribut.</span><span class="sxs-lookup"><span data-stu-id="44297-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="44297-124">Une colonne booléenne utilisée comme attribut prédictible génère trois neurones de sortie : un pour une valeur true, un pour une valeur false et un pour un état manquant ou existant.</span><span class="sxs-lookup"><span data-stu-id="44297-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="44297-125">Un attribut prédictible discret qui a plus de deux états génère un neurone de sortie pour chaque état et un neurone de sortie pour un état manquant ou existant.</span><span class="sxs-lookup"><span data-stu-id="44297-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="44297-126">Les colonnes prédictibles continues génèrent deux neurones de sortie : un pour un état manquant ou existant et un pour la valeur de la colonne continue elle-même.</span><span class="sxs-lookup"><span data-stu-id="44297-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="44297-127">Si plus de 500 neurones de sortie sont générés par l'analyse du jeu de colonnes prédictibles, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] génère un nouveau réseau dans le modèle d'exploration de données pour représenter les neurones de sortie supplémentaires.</span><span class="sxs-lookup"><span data-stu-id="44297-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="44297-128">Un neurone reçoit des entrées provenant d'autres neurones ou données, selon la couche du réseau sur laquelle il se trouve.</span><span class="sxs-lookup"><span data-stu-id="44297-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="44297-129">Un neurone d'entrée reçoit des entrées provenant des données d'origine.</span><span class="sxs-lookup"><span data-stu-id="44297-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="44297-130">Les neurones cachés et de sortie reçoivent des entrées provenant de la sortie d'autres neurones du réseau.</span><span class="sxs-lookup"><span data-stu-id="44297-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="44297-131">Les entrées établissent des relations entre les neurones et ces relations servent de chemin d'analyse pour un ensemble de cas spécifique.</span><span class="sxs-lookup"><span data-stu-id="44297-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="44297-132">Chaque entrée est dotée d'une valeur appelée *poids*, qui décrit la pertinence ou l'importance de cette entrée spécifique par rapport au neurone caché ou à celui de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="44297-133">Plus le poids affecté à une entrée est élevé, plus la valeur de celle-ci est pertinente ou importante.</span><span class="sxs-lookup"><span data-stu-id="44297-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="44297-134">Le poids peut être négatif, ce qui implique que l'entrée peut désactiver un neurone spécifique au lieu de l'activer.</span><span class="sxs-lookup"><span data-stu-id="44297-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="44297-135">La valeur de chaque entrée est multipliée par le poids pour accentuer son importance pour un neurone spécifique.</span><span class="sxs-lookup"><span data-stu-id="44297-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="44297-136">Si le poids est négatif, la multiplication de la valeur par celui-ci a pour effet de désaccentuer son importance.</span><span class="sxs-lookup"><span data-stu-id="44297-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="44297-137">Chaque neurone est doté d’une fonction non linéaire simple appelée *fonction d’activation*qui décrit la pertinence ou l’importance d’un neurone spécifique par rapport à cette couche d’un réseau neuronal.</span><span class="sxs-lookup"><span data-stu-id="44297-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="44297-138">Les neurones cachés utilisent une fonction *tangente hyperbolique* (tanh) pour leur fonction d’activation, tandis que les neurones de sortie utilisent une fonction *sigmoïde* .</span><span class="sxs-lookup"><span data-stu-id="44297-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="44297-139">Il s'agit de deux fonctions continues non linéaires qui permettent au réseau neuronal de modéliser les relations non linéaires entre les neurones d'entrée et de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="44297-140">Apprentissage des réseaux neuronaux</span><span class="sxs-lookup"><span data-stu-id="44297-140">Training Neural Networks</span></span>  
 <span data-ttu-id="44297-141">L'apprentissage d'un modèle d'exploration de données utilisant l'algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network) comporte plusieurs étapes.</span><span class="sxs-lookup"><span data-stu-id="44297-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="44297-142">Ces étapes sont fortement influencées par les valeurs que vous spécifiez pour les paramètres d'algorithme.</span><span class="sxs-lookup"><span data-stu-id="44297-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="44297-143">L'algorithme commence par évaluer et par extraire les données d'apprentissage de la source de données.</span><span class="sxs-lookup"><span data-stu-id="44297-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="44297-144">Un pourcentage des données d'apprentissage, appelé *données d'exclusion*, est réservé à l'évaluation de la précision du réseau.</span><span class="sxs-lookup"><span data-stu-id="44297-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="44297-145">Pendant le processus d'apprentissage, le réseau est évalué immédiatement après chaque itération des données d'apprentissage.</span><span class="sxs-lookup"><span data-stu-id="44297-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="44297-146">Lorsque la précision n'augmente plus, le processus d'apprentissage est interrompu.</span><span class="sxs-lookup"><span data-stu-id="44297-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="44297-147">Les valeurs des paramètres *SAMPLE_SIZE* et *HOLDOUT_PERCENTAGE* permettent de déterminer le nombre de cas à échantillonner dans les données d’apprentissage et le nombre de cas à mettre de côté pour les données d’exclusion.</span><span class="sxs-lookup"><span data-stu-id="44297-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="44297-148">La valeur du paramètre *HOLDOUT_SEED* permet de déterminer de façon aléatoire les cas individuels à mettre de côté pour les données d’exclusion.</span><span class="sxs-lookup"><span data-stu-id="44297-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="44297-149">Ces paramètres d'algorithme sont différents des propriétés HOLDOUT_SIZE et HOLDOUT_SEED qui sont appliquées à une structure d'exploration de données pour définir in jeu de données de test.</span><span class="sxs-lookup"><span data-stu-id="44297-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="44297-150">L'algorithme détermine ensuite le nombre et la complexité des réseaux pris en charge par le modèle d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="44297-151">Si le modèle d'exploration de données contient un ou plusieurs attributs qui sont utilisés uniquement pour la prédiction, l'algorithme crée un réseau unique qui représente tous ces attributs.</span><span class="sxs-lookup"><span data-stu-id="44297-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="44297-152">Si le modèle d'exploration de données contient un ou plusieurs attributs qui sont utilisés à la fois pour l'entrée et pour la prédiction, le fournisseur de l'algorithme construit un réseau pour chaque attribut.</span><span class="sxs-lookup"><span data-stu-id="44297-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="44297-153">Pour les attributs d'entrée ou les attributs prédictibles dotés de valeurs discrètes, chaque neurone d'entrée ou de sortie représente respectivement un état unique.</span><span class="sxs-lookup"><span data-stu-id="44297-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="44297-154">Pour les attributs d'entrée ou les attributs prédictibles dotés de valeurs continues, chaque neurone d'entrée ou de sortie représente respectivement la plage et la distribution de valeurs pour l'attribut.</span><span class="sxs-lookup"><span data-stu-id="44297-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="44297-155">Le nombre maximal d’états pris en charge dans les deux cas dépend de la valeur du paramètre *MAXIMUM_STATES* de l’algorithme.</span><span class="sxs-lookup"><span data-stu-id="44297-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="44297-156">Si le nombre d’états d’un attribut spécifique dépasse la valeur du paramètre d’algorithme *MAXIMUM_STATES* , les états les plus fréquents ou les plus pertinents pour cet attribut sont sélectionnés, à hauteur du nombre maximal d’états autorisé, et les états restants sont considérés comme des valeurs manquantes dans le cadre de l’analyse.</span><span class="sxs-lookup"><span data-stu-id="44297-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="44297-157">L’algorithme utilise ensuite la valeur du paramètre *HIDDEN_NODE_RATIO* pour déterminer le nombre initial de neurones à créer pour la couche cachée.</span><span class="sxs-lookup"><span data-stu-id="44297-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="44297-158">Vous pouvez attribuer la valeur 0 à *HIDDEN_NODE_RATIO* pour empêcher la création d’une couche cachée dans les réseaux générés par l’algorithme pour le modèle d’exploration de données. Le réseau neuronal est alors traité en tant que régression logistique.</span><span class="sxs-lookup"><span data-stu-id="44297-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="44297-159">Le fournisseur d'algorithme évalue de façon itérative le poids de toutes les entrées se trouvant sur le réseau au même moment, en prenant l'ensemble de données d'apprentissage qui a été auparavant mis de côté et en comparant la valeur réelle connue de chacun des cas figurant dans les données d'exclusion à la prédiction du réseau : ce processus s'appelle l' *apprentissage par lots*.</span><span class="sxs-lookup"><span data-stu-id="44297-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="44297-160">Une fois que l'algorithme a évalué l'intégralité des données d'apprentissage, l'algorithme passe en revue la valeur prédite et la valeur réelle de chaque neurone.</span><span class="sxs-lookup"><span data-stu-id="44297-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="44297-161">L'algorithme calcule, le cas échéant, le taux d'erreur et ajuste les poids attribués aux entrées de ce neurone en propageant les informations vers l'arrière, c'est-à-dire en partant des neurones de sortie et en remontant aux neurones d'entrée : ce processus s'appelle la *rétropropagation*.</span><span class="sxs-lookup"><span data-stu-id="44297-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="44297-162">L'algorithme répète ensuite le processus sur l'intégralité des données d'apprentissage.</span><span class="sxs-lookup"><span data-stu-id="44297-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="44297-163">Étant donné que l'algorithme peut prendre en charge de nombreux poids et neurones de sortie, l'algorithme du gradient conjugué est utilisé afin de guider le processus d'apprentissage pour l'attribution et l'évaluation des poids des entrées.</span><span class="sxs-lookup"><span data-stu-id="44297-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="44297-164">L'algorithme du gradient conjugué n'est pas traité dans la présente documentation.</span><span class="sxs-lookup"><span data-stu-id="44297-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="44297-165">Sélection de caractéristiques</span><span class="sxs-lookup"><span data-stu-id="44297-165">Feature Selection</span></span>  
 <span data-ttu-id="44297-166">Si le nombre d’attributs d’entrée ou d’attributs prédictibles est supérieur respectivement à la valeur du paramètre *MAXIMUM_INPUT_ATTRIBUTES* ou du paramètre *MAXIMUM_OUTPUT_ATTRIBUTES* , un algorithme de sélection des fonctionnalités est utilisé pour réduire la complexité des réseaux inclus dans le modèle d’exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="44297-167">La sélection des fonctionnalités réduit le nombre d'attributs d'entrée ou prévisibles à ceux qui sont statistiquement les plus pertinents pour le modèle.</span><span class="sxs-lookup"><span data-stu-id="44297-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="44297-168">La sélection des fonctionnalités est automatiquement utilisée par tous les algorithmes d'exploration de données [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] pour améliorer l'analyse et réduire la charge de traitement.</span><span class="sxs-lookup"><span data-stu-id="44297-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="44297-169">La méthode utilisée pour la sélection des fonctionnalités dans les modèles de réseau neuronal dépend du type de données de l'attribut.</span><span class="sxs-lookup"><span data-stu-id="44297-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="44297-170">Pour référence, le tableau suivant présente les méthodes de sélection de fonctionnalités utilisées pour les modèles de réseau neuronal et pour l'algorithme Logistic Regression qui est basé sur l'algorithme Neural Network.</span><span class="sxs-lookup"><span data-stu-id="44297-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="44297-171">Algorithm</span><span class="sxs-lookup"><span data-stu-id="44297-171">Algorithm</span></span>|<span data-ttu-id="44297-172">Méthode d'analyse</span><span class="sxs-lookup"><span data-stu-id="44297-172">Method of analysis</span></span>|<span data-ttu-id="44297-173">Commentaires</span><span class="sxs-lookup"><span data-stu-id="44297-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="44297-174">Neural Network</span><span class="sxs-lookup"><span data-stu-id="44297-174">Neural Network</span></span>|<span data-ttu-id="44297-175">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="44297-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="44297-176">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="44297-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="44297-177">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="44297-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="44297-178">Équivalent bayésien de Dirichlet avec a priori uniforme (par défaut)</span><span class="sxs-lookup"><span data-stu-id="44297-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="44297-179">L'algorithme MNN (Microsoft Neural Network, réseau neuronal de Microsoft) peut utiliser les deux méthodes de calcul de score (entropie et bayésien), tant que les données contiennent des colonnes continues.</span><span class="sxs-lookup"><span data-stu-id="44297-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="44297-180">Par défaut.</span><span class="sxs-lookup"><span data-stu-id="44297-180">Default.</span></span>|  
|<span data-ttu-id="44297-181">Logistic Regression</span><span class="sxs-lookup"><span data-stu-id="44297-181">Logistic Regression</span></span>|<span data-ttu-id="44297-182">Score d'intérêt et de pertinence</span><span class="sxs-lookup"><span data-stu-id="44297-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="44297-183">Entropie de Shannon</span><span class="sxs-lookup"><span data-stu-id="44297-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="44297-184">Bayésien avec a priori K2</span><span class="sxs-lookup"><span data-stu-id="44297-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="44297-185">Équivalent bayésien de Dirichlet avec a priori uniforme (par défaut)</span><span class="sxs-lookup"><span data-stu-id="44297-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="44297-186">Étant donné que vous ne pouvez pas passer de paramètre à cet algorithme pour contrôler le comportement de la sélection des fonctionnalités, les valeurs par défaut sont utilisées.</span><span class="sxs-lookup"><span data-stu-id="44297-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="44297-187">Par conséquent, si tous les attributs sont discrets ou discrétisés, la valeur par défaut est BDEU.</span><span class="sxs-lookup"><span data-stu-id="44297-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="44297-188">Les paramètres d'algorithme qui contrôlent la sélection des fonctionnalités d'un modèle de réseau neuronal sont MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES et MAXIMUM_STATES.</span><span class="sxs-lookup"><span data-stu-id="44297-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="44297-189">Vous pouvez également contrôler le nombre de couches masquées en définissant le paramètre HIDDEN_NODE_RATIO.</span><span class="sxs-lookup"><span data-stu-id="44297-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="44297-190">Méthodes de calcul de score</span><span class="sxs-lookup"><span data-stu-id="44297-190">Scoring Methods</span></span>  
 <span data-ttu-id="44297-191">Le*score* est une sorte de normalisation qui, dans le contexte de l'apprentissage d'un modèle de réseau neuronal, désigne le processus de conversion d'une valeur, telle qu'une étiquette de texte discrète, en une valeur qui peut être comparée avec d'autres types d'entrées et être pondérée dans le réseau.</span><span class="sxs-lookup"><span data-stu-id="44297-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="44297-192">Par exemple, si un attribut d'entrée est Gender et que les valeurs possibles sont Male et Female, et qu'un autre attribut d'entrée est Income, avec une plage de valeurs variable, les valeurs pour chaque attribut ne sont pas directement comparables, et doivent donc être encodées à une échelle commune afin que les poids puissent être calculés.</span><span class="sxs-lookup"><span data-stu-id="44297-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="44297-193">Le score est le processus qui consiste à normaliser ces entrées en valeurs numériques, plus précisément en une plage de probabilités.</span><span class="sxs-lookup"><span data-stu-id="44297-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="44297-194">Les fonctionnalités utilisées pour la normalisation permettent également de répartir la valeur d'entrée plus équitablement sur une échelle uniforme afin que les valeurs extrêmes ne déforment pas les résultats d'analyse.</span><span class="sxs-lookup"><span data-stu-id="44297-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="44297-195">Les sorties du réseau neuronal sont également encodées.</span><span class="sxs-lookup"><span data-stu-id="44297-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="44297-196">Lorsqu'il y a une cible unique pour la sortie (c'est-à-dire, la prédiction), ou plusieurs cibles utilisées uniquement pour la prédiction et non pas pour l'entrée, le modèle crée un réseau unique et il peut ne pas s'avérer nécessaire de normaliser les valeurs.</span><span class="sxs-lookup"><span data-stu-id="44297-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="44297-197">Toutefois, si plusieurs attributs sont utilisés pour l'entrée et la prédiction, le modèle doit créer plusieurs réseaux ; par conséquent, toutes les valeurs doivent être normalisées, et les sorties doivent également être encodées lorsqu'elles quittent le réseau.</span><span class="sxs-lookup"><span data-stu-id="44297-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="44297-198">L'encodage des entrées consiste à additionner chaque valeur discrète des cas d'apprentissage, et à la multiplier par son poids.</span><span class="sxs-lookup"><span data-stu-id="44297-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="44297-199">On parle alors de *somme pondérée*, laquelle est transmise à la fonction d'activation dans la couche masquée.</span><span class="sxs-lookup"><span data-stu-id="44297-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="44297-200">Un z-score est utilisé pour l'encodage :</span><span class="sxs-lookup"><span data-stu-id="44297-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="44297-201">**Valeurs discrètes**</span><span class="sxs-lookup"><span data-stu-id="44297-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="44297-202">μ = p-la probabilité antérieure d’un État</span><span class="sxs-lookup"><span data-stu-id="44297-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="44297-203">StdDev = sqrt (p (1 p))</span><span class="sxs-lookup"><span data-stu-id="44297-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="44297-204">**Valeurs continues**</span><span class="sxs-lookup"><span data-stu-id="44297-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="44297-205">Valeur présente = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="44297-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="44297-206">Aucune valeur existante =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="44297-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="44297-207">Une fois les valeurs encodées, les entrées font l'objet d'une addition pondérée, avec les contours du réseau comme poids.</span><span class="sxs-lookup"><span data-stu-id="44297-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="44297-208">L'encodage des sorties utilise la fonction sigmoïde, qui est dotée de propriétés qui la rendent très utile pour la prédiction.</span><span class="sxs-lookup"><span data-stu-id="44297-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="44297-209">L'une d'entre elles est que, indépendamment de la méthode utilisée pour mettre les valeurs d'origine à l'échelle et du fait que les valeurs sont négatives ou positives, la sortie de cette fonction est toujours une valeur comprise entre 0 et 1, ce qui convient parfaitement pour l'estimation des probabilités.</span><span class="sxs-lookup"><span data-stu-id="44297-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="44297-210">Une autre propriété utile est que la fonction sigmoïde a un effet de lissage, de sorte que lorsque les valeurs s'éloignent du point d'inflexion, la probabilité pour la valeur se rapproche de 0 ou 1, mais lentement.</span><span class="sxs-lookup"><span data-stu-id="44297-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="44297-211">Personnalisation de l'algorithme Neural Network</span><span class="sxs-lookup"><span data-stu-id="44297-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="44297-212">L'algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network) prend en charge plusieurs paramètres qui affectent le comportement, les performances et la précision du modèle d'exploration de données résultant.</span><span class="sxs-lookup"><span data-stu-id="44297-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="44297-213">Vous pouvez également modifier la manière dont le modèle traite les données en définissant des indicateurs de modélisation sur les colonnes ou des indicateurs de distribution afin de spécifier le mode de traitement des valeurs dans la colonne.</span><span class="sxs-lookup"><span data-stu-id="44297-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="44297-214">Définition des paramètres de l'algorithme</span><span class="sxs-lookup"><span data-stu-id="44297-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="44297-215">Le tableau suivant décrit les paramètres qui peuvent être utilisés avec l'algorithme MNN (Microsoft Neural Network).</span><span class="sxs-lookup"><span data-stu-id="44297-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="44297-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="44297-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="44297-217">Spécifie le taux de neurones cachés par rapport aux neurones d'entrée et de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="44297-218">La formule suivante détermine le nombre initial de neurones dans la couche masquée :</span><span class="sxs-lookup"><span data-stu-id="44297-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="44297-219">HIDDEN_NODE_RATIO \* SQRT (total neurones d’entrée \* total neurones de sortie)</span><span class="sxs-lookup"><span data-stu-id="44297-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="44297-220">La valeur par défaut est 4,0.</span><span class="sxs-lookup"><span data-stu-id="44297-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="44297-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="44297-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="44297-222">Spécifie le pourcentage de cas extraits des données d'apprentissage pour calculer l'erreur d'exclusion, qui constitue l'un des critères d'arrêt pendant l'apprentissage du modèle d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="44297-223">La valeur par défaut est 30.</span><span class="sxs-lookup"><span data-stu-id="44297-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="44297-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="44297-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="44297-225">Spécifie un nombre qui est utilisé en tant que valeur de départ du générateur de nombres pseudo-aléatoires lorsque l'algorithme détermine de façon aléatoire les données d'exclusion.</span><span class="sxs-lookup"><span data-stu-id="44297-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="44297-226">Si ce paramètre a la valeur 0, l'algorithme génère la valeur de départ en fonction du nom du modèle d'exploration de données, afin de garantir que le contenu du modèle reste inchangé pendant le retraitement.</span><span class="sxs-lookup"><span data-stu-id="44297-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="44297-227">La valeur par défaut est 0.</span><span class="sxs-lookup"><span data-stu-id="44297-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="44297-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="44297-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="44297-229">Détermine le nombre maximal d'attributs d'entrée qui peuvent être fournis à l'algorithme et au-delà duquel la sélection des fonctionnalités est utilisée.</span><span class="sxs-lookup"><span data-stu-id="44297-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="44297-230">La valeur 0 désactive la sélection des fonctionnalités pour les attributs d'entrée.</span><span class="sxs-lookup"><span data-stu-id="44297-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="44297-231">La valeur par défaut est 255.</span><span class="sxs-lookup"><span data-stu-id="44297-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="44297-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="44297-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="44297-233">Détermine le nombre maximal d'attributs de sortie qui peuvent être fournis à l'algorithme et au-delà duquel la sélection des fonctionnalités est utilisée.</span><span class="sxs-lookup"><span data-stu-id="44297-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="44297-234">La valeur 0 désactive la sélection des fonctionnalités pour les attributs de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="44297-235">La valeur par défaut est 255.</span><span class="sxs-lookup"><span data-stu-id="44297-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="44297-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="44297-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="44297-237">Spécifie le nombre maximal d'états discrets par attribut qui est pris en charge par l'algorithme.</span><span class="sxs-lookup"><span data-stu-id="44297-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="44297-238">Si le nombre d'états d'un attribut spécifique est supérieur au nombre spécifié pour ce paramètre, l'algorithme sélectionne les états les plus fréquents pour cet attribut et traite les autres comme étant absents.</span><span class="sxs-lookup"><span data-stu-id="44297-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="44297-239">La valeur par défaut est 100.</span><span class="sxs-lookup"><span data-stu-id="44297-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="44297-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="44297-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="44297-241">Spécifie le nombre de cas à utiliser pour effectuer l'apprentissage du modèle.</span><span class="sxs-lookup"><span data-stu-id="44297-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="44297-242">L'algorithme utilise soit ce nombre, soit le pourcentage du nombre total de cas non inclus dans les données d'exclusion conformément au paramètre HOLDOUT_PERCENTAGE, la plus petite valeur étant retenue.</span><span class="sxs-lookup"><span data-stu-id="44297-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="44297-243">En d'autres termes, si HOLDOUT_PERCENTAGE a la valeur 30, l'algorithme utilisera soit la valeur de ce paramètre, soit une valeur égale à 70 % du nombre total de cas, la plus petite valeur étant retenue.</span><span class="sxs-lookup"><span data-stu-id="44297-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="44297-244">La valeur par défaut est 10 000.</span><span class="sxs-lookup"><span data-stu-id="44297-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="44297-245">Indicateurs de modélisation</span><span class="sxs-lookup"><span data-stu-id="44297-245">Modeling Flags</span></span>  
 <span data-ttu-id="44297-246">Les indicateurs de modélisation suivants sont pris en charge pour une utilisation avec l'algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network).</span><span class="sxs-lookup"><span data-stu-id="44297-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="44297-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="44297-247">NOT NULL</span></span>  
 <span data-ttu-id="44297-248">Indique que la colonne ne peut pas contenir de valeur Null.</span><span class="sxs-lookup"><span data-stu-id="44297-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="44297-249">Une erreur est générée si Analysis Services rencontre une valeur NULL au cours de l'apprentissage du modèle.</span><span class="sxs-lookup"><span data-stu-id="44297-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="44297-250">S'applique aux colonnes de structure d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="44297-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="44297-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="44297-252">Indique que le modèle doit uniquement déterminer si une valeur existe ou non pour l'attribut ou si une valeur manque.</span><span class="sxs-lookup"><span data-stu-id="44297-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="44297-253">La valeur exacte n'a pas d'importance.</span><span class="sxs-lookup"><span data-stu-id="44297-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="44297-254">S'applique aux colonnes de modèle d'exploration de données.</span><span class="sxs-lookup"><span data-stu-id="44297-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="44297-255">Indicateurs de distribution</span><span class="sxs-lookup"><span data-stu-id="44297-255">Distribution Flags</span></span>  
 <span data-ttu-id="44297-256">Les indicateurs de distribution suivants sont pris en charge pour une utilisation avec l'algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network).</span><span class="sxs-lookup"><span data-stu-id="44297-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="44297-257">Les indicateurs sont uniquement utilisés comme indications pour le modèle ; si l'algorithme détecte une autre distribution, il utilisera celle-ci et non pas celle spécifiée dans l'indication.</span><span class="sxs-lookup"><span data-stu-id="44297-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="44297-258">Normal</span><span class="sxs-lookup"><span data-stu-id="44297-258">Normal</span></span>  
 <span data-ttu-id="44297-259">Indique que les valeurs de la colonne doivent être considérées comme étant la normale, ou gaussiennes.</span><span class="sxs-lookup"><span data-stu-id="44297-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="44297-260">Uniforme</span><span class="sxs-lookup"><span data-stu-id="44297-260">Uniform</span></span>  
 <span data-ttu-id="44297-261">Indique que les valeurs de la colonne doivent être considérées comme étant distribuées uniformément ; autrement dit, la probabilité de toute valeur est à peu près constante et est une fonction du nombre total de valeurs.</span><span class="sxs-lookup"><span data-stu-id="44297-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="44297-262">Log-normale</span><span class="sxs-lookup"><span data-stu-id="44297-262">Log Normal</span></span>  
 <span data-ttu-id="44297-263">Indique que les valeurs de la colonne doivent être considérées comme étant distribuées selon la courbe *log-normale* , ce qui signifie que le logarithme des valeurs est distribué normalement.</span><span class="sxs-lookup"><span data-stu-id="44297-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="44297-264">Spécifications</span><span class="sxs-lookup"><span data-stu-id="44297-264">Requirements</span></span>  
 <span data-ttu-id="44297-265">Un modèle de réseau neuronal doit contenir au moins une colonne d'entrée et une colonne de sortie.</span><span class="sxs-lookup"><span data-stu-id="44297-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="44297-266">Colonnes d'entrée et prédictibles</span><span class="sxs-lookup"><span data-stu-id="44297-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="44297-267">L'algorithme MNN ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network) prend en charge les colonnes d'entrée spécifiques et les colonnes prédictibles répertoriées dans le tableau suivant.</span><span class="sxs-lookup"><span data-stu-id="44297-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="44297-268">Colonne</span><span class="sxs-lookup"><span data-stu-id="44297-268">Column</span></span>|<span data-ttu-id="44297-269">Types de contenu</span><span class="sxs-lookup"><span data-stu-id="44297-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="44297-270">Attribut d'entrée</span><span class="sxs-lookup"><span data-stu-id="44297-270">Input attribute</span></span>|<span data-ttu-id="44297-271">Continu, Cyclique, Discret, Discrétisé, Clé, Table et Trié</span><span class="sxs-lookup"><span data-stu-id="44297-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="44297-272">Attribut prédictible</span><span class="sxs-lookup"><span data-stu-id="44297-272">Predictable attribute</span></span>|<span data-ttu-id="44297-273">Continu, Cyclique, Discret, Discrétisé et Trié</span><span class="sxs-lookup"><span data-stu-id="44297-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="44297-274">Les types de contenu Cyclique et Trié sont pris en charge, mais l'algorithme les traite comme des valeurs discrètes et n'effectue pas de traitement spécial.</span><span class="sxs-lookup"><span data-stu-id="44297-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="44297-275">Voir aussi</span><span class="sxs-lookup"><span data-stu-id="44297-275">See Also</span></span>  
 <span data-ttu-id="44297-276">[Algorithme de réseau neuronal Microsoft](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="44297-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="44297-277">[Contenu du modèle d’exploration de données pour les modèles de réseau neuronal &#40;Analysis Services d’exploration de données&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="44297-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="44297-278">Exemples de requêtes de modèle de réseau neuronal</span><span class="sxs-lookup"><span data-stu-id="44297-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
